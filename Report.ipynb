{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naïve Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The conditional probabiltys with laplace smoothing;\n",
    "\n",
    "\n",
    "\n",
    "$p ( \\mathbf { rich } | content ) = \\frac {3+1}{18}$ ,$p ( \\mathbf { not rich } | content ) = \\frac {1+1}{18}$ \n",
    "\n",
    "$p ( \\mathbf { married } | content ) = \\frac {2+1}{18}$ ,$p ( \\mathbf { unmarried } | content ) = \\frac {2+1}{18}$ \n",
    "\n",
    "$p ( \\mathbf { helathy } | content ) = \\frac {3+1}{18}$ ,$p ( \\mathbf { unhealthy } | content ) = \\frac {1+1}{18}$\n",
    "\n",
    "$p ( \\mathbf { rich } | notcontent ) = \\frac {4+1}{21}$ ,$p ( \\mathbf { not rich } | notcontent ) = \\frac {1+1}{21}$ \n",
    "\n",
    "$p ( \\mathbf { married } | notcontent ) = \\frac {1+1}{21}$ ,$p ( \\mathbf { unmarried } | notcontent ) = \\frac {4+1}{21}$ \n",
    "\n",
    "$p ( \\mathbf { helathy } | notcontent ) = \\frac {1+1}{21}$ ,$p ( \\mathbf { unhealthy } | notcontent ) = \\frac {4+1}{21}$\n",
    "\n",
    "\n",
    "\n",
    "$p ( \\mathbf { content })= \\frac {4}{9}$  $p ( \\mathbf { notcontent })=\\frac {5}{9}$\n",
    "\n",
    "### 1)\n",
    "\n",
    "test= (0,1,1)\n",
    "\n",
    "naive bayes =  $ \\frac {4}{9} x \\frac {2}{18}  x \\frac {3}{18} x \\frac {4}{18} = 0,0018$\n",
    "\n",
    "### 2)\n",
    "test = (0,1,x)\n",
    "\n",
    " $ \\frac {4}{9} x \\frac {2}{18}  x \\frac {3}{18} x \\frac {4}{18} + \\frac {4}{9} x \\frac {2}{18}  x \\frac {3}{18} x \\frac {2}{18} = 0,0027$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART II:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   INTRODUCTIONS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes is a simple technique for constructing classifiers: models that assign class labels to problem instances, represented as vectors of feature values, where the class labels are drawn from some finite set. There is not a single algorithm for training such classifiers, but a family of algorithms based on a common principle: all naive Bayes classifiers assume that the value of a particular feature is independent of the value of any other feature, given the class variable.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I created the algorithm according to the basic variables while coding. The first of these is ngram. I kept this value in the bow_gram variable. Here I determined how many words to choose for the Bag of Words algorithm. I made using stop words optional. When I used stop words, the success rate of the model increased by 1-2%. You can find the results and ngram tables below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What is the N-gram Based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unigram "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy:  83.29166666666666"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Accuracy:  87.29166666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing effect of the words on prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.In Unigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_type = \"presence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Word/WordPairs  Frequency\n",
    "0          snack   2.942399\n",
    "1             4x   1.982585\n",
    "2             s2   1.810315\n",
    "3      sunscreen   1.796337\n",
    "4       wildlife   1.748248\n",
    "5       hesitate   1.702970\n",
    "6    backgrounds   1.674810\n",
    "7          oasis   1.672294\n",
    "8           tivo   1.652419\n",
    "9           fold   1.594980"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_type = \"absence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Word/WordPairs  Frequency\n",
    "0    uninstalled   3.844397\n",
    "1            e18   3.816932\n",
    "2         tossed   2.524731\n",
    "3           blah   2.344533\n",
    "4       rambling   2.052342\n",
    "5            faq   1.994579\n",
    "6         seagal   1.982785\n",
    "7        haitian   1.779322\n",
    "8            nis   1.748462\n",
    "9           mone   1.681558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.In Bigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_type = \"presence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            Word/WordPairs  Frequency\n",
    "0             great buy   2.609978\n",
    "1            nancy drew   2.186287\n",
    "2           great value   1.785551\n",
    "3  definitely recommend   1.781253\n",
    "4         timely manner   1.667875\n",
    "5             in timely   1.655153\n",
    "6      definitely worth   1.638642\n",
    "7               fits my   1.487171\n",
    "8          product easy   1.455214\n",
    "9              all ages   1.431996"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "word_type = \"absence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "            Word/WordPairs  Frequency\n",
    "0       very disappointed   6.558583\n",
    "1              money back   3.380378\n",
    "2                 of junk   3.072244\n",
    "3              what waste   2.519676\n",
    "4              and boring   2.048000\n",
    "5     what disappointment   2.013054\n",
    "6            the warranty   1.903294\n",
    "7              for refund   1.833572\n",
    "8           returned this   1.832326\n",
    "9  extremely disappointed   1.770626"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### StopWords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common practice is to remove stopwords for the set of words that are used as features, since they do not contribute to a specific class, therefore overshadowing words that actually carry more sentimental information. You can image that words such as ‘since’, ‘all’ and ‘however’ occur roughly as often in negative as in positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.In Unigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_words=\"english\", word_type=\"presence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Word/WordPairs  Frequency\n",
    "0          snack   3.229496\n",
    "1             4x   2.225892\n",
    "2      sunscreen   2.044457\n",
    "3             s2   2.022079\n",
    "4       wildlife   2.000286\n",
    "5       hesitate   1.920996\n",
    "6          oasis   1.886586\n",
    "7    backgrounds   1.856977\n",
    "8           fold   1.784321\n",
    "9           tivo   1.758151"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_words=\"english\", word_type = \"absence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    Word/WordPairs  Frequency\n",
    "0    uninstalled   4.333591\n",
    "1            e18   4.129571\n",
    "2         tossed   2.927498\n",
    "3           blah   2.663093\n",
    "4       rambling   2.290647\n",
    "5            faq   2.219065\n",
    "6         seagal   2.217544\n",
    "7        haitian   2.002169\n",
    "8            nis   1.979014\n",
    "9           mone   1.848270"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.In Bigram"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_words=\"english\", word_type=\"presence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "        Word/WordPairs  Frequency\n",
    "0         great buy   3.393760\n",
    "1      love product   3.004921\n",
    "2        nancy drew   2.770606\n",
    "3       great value   2.233675\n",
    "4          use easy   1.999791\n",
    "5     timely manner   1.956985\n",
    "6      use accurate   1.942439\n",
    "7   especially like   1.922881\n",
    "8   excellent value   1.791336\n",
    "9  great collection   1.713285"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stop_words=\"english\", word_type=\"absence\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "             Word/WordPairs  Frequency\n",
    "0              piece junk   3.141596\n",
    "1  extremely disappointed   2.254617\n",
    "2            wasted money   2.081284\n",
    "3           cheap plastic   2.077909\n",
    "4           received item   2.044822\n",
    "5                 did buy   1.830211\n",
    "6    disappointed product   1.787453\n",
    "7          poorly written   1.735659\n",
    "8      big disappointment   1.729794\n",
    "9          spyware doctor   1.701558"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using stop words has been more beneficial for unigram. Because in fact too many words affected the result, regardless of whether the sentence was positive or not. Thanks to the stop words, I got rid of them. There was no serious difference between whether I omitted the stop words or not, as I kept the double word groups in Bigram. Since nonsense words were also included in the list in terms of frequency of use in unigram, the Accuracy value was lower. I got a higher Accuracy in the bigram."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
